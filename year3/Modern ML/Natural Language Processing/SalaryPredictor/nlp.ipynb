{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c391e822-aad8-4251-ab88-c95ba3660722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from tqdm import tqdm\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fcf484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU = torch.device('cpu')\n",
    "GPU = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444bd07",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4bef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    if isinstance(data, dict):\n",
    "        return dict((k, to_device(v, device)) for k, v in data.items())\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000c18c",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e05617",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'Train_rev1/Train_rev1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30188a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalaryDataset(Dataset):\n",
    "    # token threshold\n",
    "    MIN_COUNT = 10\n",
    "\n",
    "    # special tokens for unknown and empty words\n",
    "    PAD, UNK = 'PAD', 'UNK'\n",
    "    PAD_IX, UNK_IX = 0, 1\n",
    "\n",
    "    TEXT_COLS = ['Title', 'FullDescription']\n",
    "    CATEGORIAL_COLS = ['Category', 'Company', 'LocationNormalized', 'ContractType', 'ContractTime']\n",
    "    TARGET_COL = 'Log1pSalary'\n",
    "\n",
    "    MAX_TITLE_LENGHT = 20\n",
    "    MAX_DESC_LENGHT = 500\n",
    "\n",
    "    def _process_data(self):\n",
    "        # use logarithm to get rid of bad distribution\n",
    "        self._data[self.TARGET_COL] = np.log1p(self._data['SalaryNormalized']).astype('float32')\n",
    "\n",
    "        # cast missing values to string \"NaN\"\n",
    "        self._data[self.CATEGORIAL_COLS] = self._data[self.CATEGORIAL_COLS].fillna('NaN')\n",
    "        self._data[self.TEXT_COLS] = self._data[self.TEXT_COLS].fillna('NaN')\n",
    "\n",
    "        # convert text fields to space-separated string of tokens\n",
    "        tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "        self._data[self.TEXT_COLS] = self._data[self.TEXT_COLS].applymap(lambda x: \" \".join(tokenizer.tokenize(x.lower())))\n",
    "\n",
    "        # count how many times does each token occur in both \"Title\" and \"FullDescription\" in total\n",
    "        for col in self._data[self.TEXT_COLS]:\n",
    "            for line in self._data[col].values:\n",
    "                self._tok_cntr.update(line.split(\" \"))\n",
    "\n",
    "        # get a list of all tokens that occur at least MIN_COUNT times\n",
    "        self._tokens = sorted(t for t, c in self._tok_cntr.items() if c >= self.MIN_COUNT)\n",
    "\n",
    "        # add a special tokens for unknown and empty words\n",
    "        self._tokens = [self.PAD, self.UNK] + self._tokens\n",
    "\n",
    "        # build an inverse token index: a dictionary from token(string) to it's index in tokens(int)\n",
    "        self._token_to_id = {t: i for i, t in enumerate(self._tokens)}\n",
    "\n",
    "        # we only consider top-1k most frequent companies to minimize memory usage\n",
    "        top_companies, top_counts = zip(*Counter(self._data['Company']).most_common(1000))\n",
    "        recognized_companies = set(top_companies)\n",
    "        self._data[\"Company\"] = self._data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "\n",
    "        # encode the categorical data we have\n",
    "        self._categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False).fit(self._data[self.CATEGORIAL_COLS].apply(dict, axis=1))\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        self._data = pd.read_csv(path)\n",
    "        self._tok_cntr = Counter()\n",
    "        self._process_data()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self._data[i:i+1]\n",
    "\n",
    "        title = row['Title'].values[0]\n",
    "        desc = row['FullDescription'].values[0]\n",
    "\n",
    "        title_vals_encoded = [self._token_to_id.get(tok, self.UNK_IX) for tok in str.split(title, ' ')]\n",
    "        desc_vals_encoded = [self._token_to_id.get(tok, self.UNK_IX) for tok in str.split(desc, ' ')]\n",
    "\n",
    "        return {\n",
    "            'Title': title_vals_encoded,\n",
    "            'FullDescription': desc_vals_encoded,\n",
    "            self.TARGET_COL: row[self.TARGET_COL].values[0],\n",
    "            'Categorical': self._categorical_vectorizer.transform(row[self.CATEGORIAL_COLS].apply(dict, axis=1)).flatten().tolist()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c317ce",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "078133b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SalaryDataset(DATA_DIR)\n",
    "NUM_TOKENS = len(dataset._tokens)\n",
    "NUM_CAT_FEATURES = len(dataset._categorical_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba98da7",
   "metadata": {},
   "source": [
    "Split to train / test / validation subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e512692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sizes is 0.7, 0.1, 0.2 of the original data size, respectively\n",
    "train_size = round(len(dataset)*0.7)\n",
    "val_size = round((len(dataset) - train_size) * (1/3))\n",
    "test_size = (len(dataset) - train_size) - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], torch.Generator().manual_seed(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "113b370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\" Converts batch values to tensors and adds padding \"\"\"\n",
    "    collated = dict(zip(data[0].keys(), [[], [], [], []]))\n",
    "\n",
    "    for d in data:\n",
    "        for k, v in d.items():\n",
    "            if k == 'Title':\n",
    "                v.extend([SalaryDataset.PAD_IX] * (SalaryDataset.MAX_TITLE_LENGHT - len(v))) # padding\n",
    "                v = v[:SalaryDataset.MAX_TITLE_LENGHT]\n",
    "                collated[k].append(v)\n",
    "            elif k == 'FullDescription':\n",
    "                v.extend([SalaryDataset.PAD_IX] * (SalaryDataset.MAX_DESC_LENGHT - len(v)))  # padding\n",
    "                v = v[:SalaryDataset.MAX_DESC_LENGHT]\n",
    "                collated[k].append(v)\n",
    "            else:\n",
    "                collated[k].append(v)\n",
    "\n",
    "    for k, v in collated.items():\n",
    "        t = torch.float32 if k in [SalaryDataset.TARGET_COL, 'Categorical'] else torch.int32\n",
    "        collated[k] = torch.as_tensor(v, dtype=t)\n",
    "        \n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442eff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4ed24",
   "metadata": {},
   "source": [
    "# Deep learning, finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9559e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalaryPredictor(nn.Module):\n",
    "    def __init__(self, n_tokens=NUM_TOKENS, n_cat_features=NUM_CAT_FEATURES, hid_size=8):\n",
    "        super().__init__()\n",
    "        self.n_tokens = n_tokens\n",
    "        self.n_cat_features = n_cat_features\n",
    "        self.hid_size = hid_size\n",
    "        self.embedder = nn.Embedding(n_tokens, hid_size)\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Conv1d(hid_size, hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Conv1d(hid_size, hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.categorical_encoder = nn.Sequential(\n",
    "            nn.Linear(n_cat_features, hid_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size * 2, hid_size * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final_predictor = nn.Sequential(\n",
    "            nn.Linear(hid_size * 4, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "        title_features = self.title_encoder(title_embeddings).squeeze()\n",
    "\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "        description_features = self.description_encoder(description_embeddings).squeeze()\n",
    "\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "\n",
    "        features = torch.cat([title_features, description_features, categorical_features], dim=1)\n",
    "        return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59039406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device=None):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "\n",
    "    loader = val_loader if not device else DeviceDataLoader(val_loader, device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            pred = model(batch)\n",
    "            squared_error += torch.mean(torch.square(pred - batch[SalaryDataset.TARGET_COL]))\n",
    "            abs_error += torch.mean(torch.abs(pred - batch[SalaryDataset.TARGET_COL]))\n",
    "            num_samples += len(batch)\n",
    "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
    "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
    "\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d02852ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoches=5, device=None, criterion=nn.MSELoss(reduction='mean')):\n",
    "    loader = train_loader\n",
    "\n",
    "    if device:\n",
    "        model.to(device)\n",
    "        loader = DeviceDataLoader(train_loader, device)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        model.train()\n",
    "        for batch in tqdm(loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, batch[SalaryDataset.TARGET_COL])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        mse, mae = evaluate(model, device)\n",
    "        print(f'Epoch: {epoch+1} | Loss: {loss.item()} | Validation: MSE={mse}/MAE={mae}')\n",
    "\n",
    "    if device:\n",
    "        model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a8e7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device=None):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    loader = test_loader\n",
    "\n",
    "    if device:\n",
    "        model.to(device)\n",
    "        loader = DeviceDataLoader(test_loader, device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x in loader:\n",
    "            pred = model(x)\n",
    "            squared_error += torch.mean(torch.square(pred - x[SalaryDataset.TARGET_COL]))\n",
    "            abs_error += torch.mean(torch.abs(pred - x[SalaryDataset.TARGET_COL]))\n",
    "            num_samples += len(x)\n",
    "\n",
    "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
    "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
    "\n",
    "    if device:\n",
    "        model.cpu()\n",
    "\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4024e",
   "metadata": {},
   "source": [
    "# 1. Развейте СNN архитектуру"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85068f",
   "metadata": {},
   "source": [
    "Начальные показатели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b6d49b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:02<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.18208062648773193 | Validation: MSE=0.7155646483103434/MAE=0.4108586311340332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [02:58<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.13678191602230072 | Validation: MSE=0.6288680632909139/MAE=0.38589104016621906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:00<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.1166573241353035 | Validation: MSE=0.557366689046224/MAE=0.3631752332051595\n"
     ]
    }
   ],
   "source": [
    "model = SalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), epoches=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36b349",
   "metadata": {},
   "source": [
    "With BatchNorm and LayerNorm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3aebd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLayerNormSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.LayerNorm(SalaryDataset.MAX_TITLE_LENGHT-1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.LayerNorm(SalaryDataset.MAX_DESC_LENGHT-1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.categorical_encoder = nn.Sequential(\n",
    "            nn.Linear(self.n_cat_features, self.hid_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(self.hid_size * 2),\n",
    "            nn.Linear(self.hid_size * 2, self.hid_size * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final_predictor = nn.Sequential(\n",
    "            nn.Linear(self.hid_size * 4, self.hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(self.hid_size),\n",
    "            nn.Linear(self.hid_size, 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011deeaa",
   "metadata": {},
   "source": [
    "С добавлением BatchNorm и LayerNorm исходная сеть начала показывать результаты намного лучше, особенно это заметно по значениям MSE/MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b623a7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:04<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.15004849433898926 | Validation: MSE=0.04239973425865173/MAE=0.08052261173725128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:08<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.11695276945829391 | Validation: MSE=0.0338154137134552/MAE=0.07059418161710103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:07<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.09838034957647324 | Validation: MSE=0.028148611386617024/MAE=0.06330659985542297\n"
     ]
    }
   ],
   "source": [
    "model = BatchLayerNormSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), epoches=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bb977",
   "metadata": {},
   "source": [
    "With parrallel conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6272bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParrallelConvSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.title_encoder2 = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=4),\n",
    "            nn.Dropout(p=0.33),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.description_encoder2 = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=4),\n",
    "            nn.Dropout(p=0.33),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.final_predictor = nn.Sequential(\n",
    "            nn.Linear(self.hid_size * 6, self.hid_size * 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_size * 3, int(self.hid_size * 1.5)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(self.hid_size * 1.5), 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "            title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "\n",
    "            title_features = self.title_encoder(title_embeddings).squeeze()\n",
    "            title_features2 = self.title_encoder2(title_embeddings).squeeze()\n",
    "\n",
    "            description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "\n",
    "            description_features = self.description_encoder(description_embeddings).squeeze()\n",
    "            description_features2 = self.description_encoder2(description_embeddings).squeeze()\n",
    "\n",
    "            categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "\n",
    "            title_features = torch.cat((title_features, title_features2), dim=1)\n",
    "            description_features = torch.cat((description_features, description_features2), dim=1)\n",
    "            features = torch.cat([title_features, description_features, categorical_features], dim=1)\n",
    "            \n",
    "            return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6c0af",
   "metadata": {},
   "source": [
    "При добавлении в исходную сеть параллельных сверточных слоев заметно лишь небольшое улучшение MSE и MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecd3d300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:25<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.15557654201984406 | Validation: MSE=0.8001741568247477/MAE=0.436512549718221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:23<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.13028569519519806 | Validation: MSE=0.5427242517471313/MAE=0.35794345537821454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:20<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.12404994666576385 | Validation: MSE=0.4783267180124919/MAE=0.3356606165568034\n"
     ]
    }
   ],
   "source": [
    "model = ParrallelConvSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6a5f7",
   "metadata": {},
   "source": [
    "Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b638acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedder = nn.Embedding(self.n_tokens, self.hid_size * 2)  # change output size\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size * 2, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),                              # add BatchNorm\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),     # add second convolution\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.title_encoder2 = nn.Sequential(                            # parrallel encoder for title\n",
    "            nn.Conv1d(self.hid_size * 2, self.hid_size, kernel_size=4),\n",
    "            nn.Dropout(p=0.33),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size * 2, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),                              # add BatchNorm\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),     # add second convolution\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.description_encoder2 = nn.Sequential(                      # parrallel encoder for description\n",
    "            nn.Conv1d(self.hid_size * 2, self.hid_size, kernel_size=4),\n",
    "            nn.Dropout(p=0.33),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.categorical_encoder = nn.Sequential(                       # change sizes of Linear layers\n",
    "            nn.Linear(self.n_cat_features, self.hid_size * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(self.hid_size * 4),                            # add LayerNorm\n",
    "            nn.Linear(self.hid_size * 4, self.hid_size * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final_predictor = nn.Sequential(                           # add more linear layers\n",
    "            nn.Linear(self.hid_size * 6, self.hid_size * 3),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(self.hid_size * 3),                            # add LayerNorm\n",
    "            nn.Linear(self.hid_size * 3, int(self.hid_size * 1.5)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(int(self.hid_size * 1.5)),                   # add BatchNorm\n",
    "            nn.Linear(int(self.hid_size * 1.5), 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "\n",
    "        title_features = self.title_encoder(title_embeddings).squeeze()\n",
    "        title_features2 = self.title_encoder2(title_embeddings).squeeze()\n",
    "\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "\n",
    "        description_features = self.description_encoder(description_embeddings).squeeze()\n",
    "        description_features2 = self.description_encoder2(description_embeddings).squeeze()\n",
    "\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "\n",
    "        title_features = torch.cat((title_features, title_features2), dim=1)\n",
    "        description_features = torch.cat((description_features, description_features2), dim=1)\n",
    "        features = torch.cat((title_features, description_features, categorical_features), dim=1)\n",
    "        \n",
    "        return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10c40e",
   "metadata": {},
   "source": [
    "Используя и нормализацию и параллельные энкодеры получилось добиться результатов еще лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db0720ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:40<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.11877128481864929 | Validation: MSE=0.05135465164979299/MAE=0.0930001934369405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:42<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.10364072024822235 | Validation: MSE=0.023742082218329113/MAE=0.057655890782674156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:37<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.10311577469110489 | Validation: MSE=0.0232709397872289/MAE=0.0569325586160024\n"
     ]
    }
   ],
   "source": [
    "model = NewSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e205f114",
   "metadata": {},
   "source": [
    "# Раняя остановка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12b2dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(lst: list):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "def delta(x):\n",
    "    def delta_impl(y):\n",
    "        return x - y\n",
    "    return delta_impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d58113fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop_train(model, optimizer, epoches=5, device=None, criterion=nn.MSELoss(reduction='mean')):\n",
    "    EXIT_CRITERION = 0.01\n",
    "    loader = train_loader\n",
    "    outputs: dict[str, list] = {}\n",
    "\n",
    "    if device:\n",
    "        model.to(device)\n",
    "        loader = DeviceDataLoader(train_loader, device)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        model.train()\n",
    "        for batch in tqdm(loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, batch[SalaryDataset.TARGET_COL])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        mse, mae = evaluate(model, device)\n",
    "        print(f'Epoch: {epoch+1} | Loss: {loss.item()} | Validation: MSE={mse}/MAE={mae}')\n",
    "\n",
    "        # mean loss, mse, mae from last 5 iterations\n",
    "        means = {k: avg(v[-5:]) for k,v in outputs.items()}\n",
    "        deltas = [means['Loss'] - loss.item(), means['mse'] - mse, means['mae'] - mae]\n",
    "\n",
    "        # if any of the deltas smaller than EXIT_CRITERION\n",
    "        if any(x < EXIT_CRITERION for x in deltas):\n",
    "            print('Stop criterion achieved')\n",
    "            return\n",
    "\n",
    "        outputs['Losses'].append(loss.item())\n",
    "        outputs['mse'].append(mse)\n",
    "        outputs['mae'].append(mae)\n",
    "\n",
    "    if device:\n",
    "        model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10006965",
   "metadata": {},
   "source": [
    "# 2. Pooling слои стандартные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff37ed",
   "metadata": {},
   "source": [
    "### Как работает pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf693b",
   "metadata": {},
   "source": [
    "Pooling слой это, фактически, фильтр заданного размера (kernel_size) который \"скользит\" по входной матрице и выдает число, в зависимости от алгоритма. Например, MaxPooling запишет в выходную матрицу только максимальное число из попавших в окно фильтра. Далее окно сдвигается на заданную величину (stride) и алгоритм повторяется, пока не будет покрыта вся входная матрица. AvgPooling вместо максимального числа выдает среднее среди тех, что попали в окно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d3cfc",
   "metadata": {},
   "source": [
    "Максимум по временной компоненте, для каждой фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e53ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU()\n",
    "            # nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU()\n",
    "            # nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.final_predictor = nn.Sequential(\n",
    "            nn.Linear(534, self.hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        max_pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "        title_features = self.title_encoder(title_embeddings).squeeze().permute(0, 2, 1)\n",
    "        title_features = max_pool(title_features).squeeze() # Max pooling\n",
    "\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "        description_features = self.description_encoder(description_embeddings).squeeze().permute(0, 2, 1)\n",
    "        description_features = max_pool(description_features).squeeze() # Max pooling\n",
    "\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "\n",
    "        features = torch.cat((title_features, description_features, categorical_features), dim=1)\n",
    "        return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766e641",
   "metadata": {},
   "source": [
    "Использование MaxPool дало лишь немного меньшее значение loss по сравнению с исходной сетью, остальные параметры ухудшились, особенно MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "605a6806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [02:59<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.13812217116355896 | Validation: MSE=0.7932665348052979/MAE=0.434911052385966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:02<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.10082586109638214 | Validation: MSE=0.8566711743672689/MAE=0.45446570714314777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:02<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.10044009983539581 | Validation: MSE=0.8534278869628906/MAE=0.4546033938725789\n"
     ]
    }
   ],
   "source": [
    "model = MaxPoolSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d60c2e",
   "metadata": {},
   "source": [
    "Среднее по временной компоненте (исключая PAD символы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c6f5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPoolSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            # nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.title_avgPooler = nn.AvgPool1d(19, stride=19, count_include_pad=False)\n",
    "\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            # nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.desc_avgPooler = nn.AvgPool1d(499, stride=499, count_include_pad=False)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Works like AdaptiveAvgPool1d(output_size=1), but have count_include_pad parameter, which should ignore PAD indices (maybe...)\n",
    "        # nn.AvgPool1d(kernel_size=input_size-(output_size-1)*stride, stride=input_size/output_size, count_include_pad=False)\n",
    "\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "        title_features = self.title_encoder(title_embeddings).squeeze()\n",
    "        title_features = self.title_avgPooler(title_features).squeeze() # Average pooling\n",
    "\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "        description_features = self.description_encoder(description_embeddings).squeeze()\n",
    "        description_features = self.desc_avgPooler(description_features).squeeze() # Average pooling\n",
    "\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "        features = torch.cat((title_features, description_features, categorical_features), dim=1)\n",
    "        return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4e791",
   "metadata": {},
   "source": [
    "Другое дело с Average Pooling'ом. С ним сеть показывает отличные результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a69b4a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:01<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.15778055787086487 | Validation: MSE=0.03812142958243688/MAE=0.07580889264742534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [02:56<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.12668080627918243 | Validation: MSE=0.028624010582764942/MAE=0.06433808306852977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [02:57<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.1180286556482315 | Validation: MSE=0.02499747524658839/MAE=0.05994426210721334\n"
     ]
    }
   ],
   "source": [
    "model = AvgPoolSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f96c62",
   "metadata": {},
   "source": [
    "# 3. Используйте предобученные эмбеддинги"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657ccd5",
   "metadata": {},
   "source": [
    "Загрузите предобученные эмбеддинги с помощью gensim.downloader.load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aac66e",
   "metadata": {},
   "source": [
    "Предобученный эмбеддинг с замороженными весами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e54fd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedFrozenSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self, wordvec: torch.FloatTensor, hid_size=100):\n",
    "        super().__init__(hid_size=100)\n",
    "        self.embedder = nn.Embedding.from_pretrained(wordvec, freeze=True) # Freezed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "775f604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c94da",
   "metadata": {},
   "source": [
    "Показывает убывающие низкие значения для loss, но ошибка все еще остается очень высокой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14f61688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [06:05<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.09762098640203476 | Validation: MSE=1.3745265007019043/MAE=0.580855925877889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [06:07<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.08756406605243683 | Validation: MSE=1.3878563245137532/MAE=0.5844357013702393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [06:09<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.0801367312669754 | Validation: MSE=1.358284632364909/MAE=0.5782964626948038\n"
     ]
    }
   ],
   "source": [
    "model = PretrainedFrozenSalaryPredictor(torch.FloatTensor(kv.vectors))\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26756fdd",
   "metadata": {},
   "source": [
    "Предобученный эмбеддинг с обучаемыми весами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b8899b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedEmbedSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self, wordvec: torch.FloatTensor, hid_size=100):\n",
    "        super().__init__(hid_size=hid_size)\n",
    "        self.embedder = nn.Embedding.from_pretrained(wordvec, freeze=False) # Not freezed weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57408c04",
   "metadata": {},
   "source": [
    "С незамороженными весами ситуцаия таже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c4b7d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [12:00<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.10760591924190521 | Validation: MSE=1.3251994450887044/MAE=0.5707526604334513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [12:06<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.09182409197092056 | Validation: MSE=1.3764427502950032/MAE=0.5826658805211385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [12:02<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.06739814579486847 | Validation: MSE=1.4146699905395508/MAE=0.5909947554270426\n"
     ]
    }
   ],
   "source": [
    "model = PretrainedEmbedSalaryPredictor(torch.FloatTensor(kv.vectors))\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a3b3b",
   "metadata": {},
   "source": [
    "# 4. Замените сверточные слои на рекуррентные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4aeba",
   "metadata": {},
   "source": [
    "With LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "368d6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self, bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.title_lstm = nn.LSTM(20, self.hid_size, bidirectional=bidirectional)\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            # nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.desc_lstm = nn.LSTM(500, self.hid_size, bidirectional=bidirectional)\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            # nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "        lstm_out, (hn, cn) = self.title_lstm(title_embeddings) # LSTM\n",
    "\n",
    "        title_features = self.title_encoder(lstm_out).squeeze()\n",
    "\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "        lstm_out, (hn, cn) = self.desc_lstm(description_embeddings) # LSTM\n",
    "\n",
    "        description_features = self.description_encoder(lstm_out).squeeze()\n",
    "\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "\n",
    "        features = torch.cat(\n",
    "            [title_features, description_features, categorical_features], dim=1)\n",
    "        \n",
    "        return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28346f28",
   "metadata": {},
   "source": [
    "bidirectional = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8d536a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:20<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.18705296516418457 | Validation: MSE=0.29569631814956665/MAE=0.25262222687403363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:15<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.15957172214984894 | Validation: MSE=0.252300759156545/MAE=0.232072651386261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:15<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.13809098303318024 | Validation: MSE=0.15904400746027628/MAE=0.1786778966585795\n"
     ]
    }
   ],
   "source": [
    "model = LSTMSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb20cf",
   "metadata": {},
   "source": [
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84c45f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:57<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.15448284149169922 | Validation: MSE=0.780951976776123/MAE=0.4307365417480469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:59<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.14052137732505798 | Validation: MSE=0.7810777823130289/MAE=0.43112874031066895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:59<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.13719868659973145 | Validation: MSE=0.810487429300944/MAE=0.4400142828623454\n"
     ]
    }
   ],
   "source": [
    "model = LSTMSalaryPredictor(bidirectional=True)\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72444b00",
   "metadata": {},
   "source": [
    "With GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a7d283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self, bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.title_gru = nn.GRU(20, self.hid_size, bidirectional=bidirectional)\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            # nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.desc_gru = nn.GRU(500, self.hid_size, bidirectional=bidirectional)\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            # nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "        gru_out, hn = self.title_gru(title_embeddings) # GRU\n",
    "\n",
    "        title_features = self.title_encoder(gru_out).squeeze()\n",
    "\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "        gru_out, hn = self.desc_gru(description_embeddings) # GRU\n",
    "\n",
    "        description_features = self.description_encoder(gru_out).squeeze()\n",
    "\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "\n",
    "        features = torch.cat([title_features, description_features, categorical_features], dim=1)\n",
    "        \n",
    "        return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da9198",
   "metadata": {},
   "source": [
    "bidirectional = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e7ba1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:19<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.13911426067352295 | Validation: MSE=0.3445599476496379/MAE=0.2771459420522054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:20<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.13552053272724152 | Validation: MSE=0.35835285981496173/MAE=0.28341031074523926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [03:20<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.13401997089385986 | Validation: MSE=0.3018399675687154/MAE=0.25797730684280396\n"
     ]
    }
   ],
   "source": [
    "model = GRUSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece779c",
   "metadata": {},
   "source": [
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0551313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:10<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.1441812962293625 | Validation: MSE=0.32176361481348675/MAE=0.26619766155878705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:09<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.13940666615962982 | Validation: MSE=0.3248337507247925/MAE=0.26810304323832196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:09<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.13403506577014923 | Validation: MSE=0.35299789905548096/MAE=0.28113792339960736\n"
     ]
    }
   ],
   "source": [
    "model = GRUSalaryPredictor(bidirectional=True)\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971a61b",
   "metadata": {},
   "source": [
    "LSTM отрабатывает лучше чем GRU, но по сравнению с другими сетями результат оставляет желать лучшего"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
