{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Modern_ML.Lab_2.Object_Detection.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"-BAAJg31ij1L"},"source":["# <center> Майнор \"Интеллектуальный анализ данных\" </center>"]},{"cell_type":"markdown","metadata":{"id":"C_J6UXd1ij1Y"},"source":["# <center> Курс \"Современные методы машинного обучения\" </center>"]},{"cell_type":"markdown","metadata":{"id":"TYtVG13Jij1b"},"source":["# <center> Лабораторная работа №2. Object Detection. </center>"]},{"cell_type":"markdown","metadata":{"id":"cO8D4VK7ij1d"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"rcdM-buhij1f"},"source":["import pandas as pd\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import glob\n","import cv2\n","import os\n","import torchvision\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import auc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A7uDmELAij1l"},"source":["Install library for processing the labeling\n","```bash\n","pip install xmltodict\n","```"]},{"cell_type":"code","metadata":{"id":"4cFYT77dij1n"},"source":["import xmltodict, json"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mrE0tJX0ij1r"},"source":["В этом задании потребуется обучить детектор фруктов на изображении. Данные находятся в архиве `data.zip`. Данные уже поделены на train и test."]},{"cell_type":"markdown","metadata":{"id":"1wJQFd9cij1x"},"source":["Датасет для трех классов."]},{"cell_type":"code","metadata":{"id":"HYiR9cEqij19"},"source":["class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n","\n","class FruitDataset(Dataset):\n","    def __init__(self, data_dir, transform=None):\n","        self.images = []\n","        self.annotations = []\n","        self.transform = transform\n","        for annotation in glob.glob(data_dir + \"/*xml\"):\n","            image_fname = os.path.splitext(annotation)[0] + \".jpg\"\n","            self.images.append(cv2.cvtColor(cv2.imread(image_fname), cv2.COLOR_BGR2RGB))\n","            with open(annotation) as f:\n","                annotation_dict = xmltodict.parse(f.read())\n","            bboxes = []\n","            labels = []\n","            objects = annotation_dict[\"annotation\"][\"object\"]\n","            if not isinstance(objects, list):\n","                objects = [objects]\n","            for obj in objects:\n","                bndbox = obj[\"bndbox\"]\n","                bbox = [bndbox[\"xmin\"], bndbox[\"ymin\"], bndbox[\"xmax\"], bndbox[\"ymax\"]]\n","                bbox = list(map(int, bbox))\n","                bboxes.append(torch.tensor(bbox))\n","                labels.append(class2tag[obj[\"name\"]])\n","            self.annotations.append(\n","                {\"boxes\": torch.stack(bboxes).float(), \"labels\": torch.tensor(labels)}\n","            )\n","\n","    def __getitem__(self, i):\n","        if self.transform:\n","            # the following code is correct if you use albumentations\n","            # if you use torchvision transforms you have to modify it =)\n","            res = self.transform(\n","                image=self.images[i],\n","                bboxes=self.annotations[i][\"boxes\"],\n","                labels=self.annotations[i][\"labels\"],\n","            )\n","            return res[\"image\"], {\n","                \"boxes\": torch.tensor(res[\"bboxes\"]),\n","                \"labels\": torch.tensor(res[\"labels\"]),\n","            }\n","        else:\n","            return self.images[i], self.annotations[i]\n","\n","    def __len__(self):\n","        return len(self.images)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fZ9A2Vsxij2A"},"source":["<br>  \n","<br>  \n","Функции для вычисления mAP."]},{"cell_type":"code","metadata":{"id":"4kNeTMzwij2C"},"source":["def intersection_over_union(dt_bbox, gt_bbox):\n","    \"\"\"\n","    Intersection over Union between two bboxes\n","    :param dt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n","    :param gt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n","    :return : intersection over union\n","    \"\"\"\n","\n","    ## TODO YOUR CODE\n","\n","    intersection_bbox = np.array(\n","        [\n","            max(dt_bbox[0], gt_bbox[0]),\n","            max(dt_bbox[1], gt_bbox[1]),\n","            min(dt_bbox[2], gt_bbox[2]),\n","            min(dt_bbox[3], gt_bbox[3]),\n","        ]\n","    )\n","\n","    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) * max(\n","        intersection_bbox[3] - intersection_bbox[1], 0\n","    )\n","    area_dt = (dt_bbox[2] - dt_bbox[0]) * (dt_bbox[3] - dt_bbox[1])\n","    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n","\n","    union_area = area_dt + area_gt - intersection_area\n","\n","    iou = intersection_area / union_area\n","    return iou\n","\n","def evaluate_sample(target_pred, target_true, iou_threshold=0.5):\n","    gt_bboxes = target_true[\"boxes\"].numpy()\n","    gt_labels = target_true[\"labels\"].numpy()\n","\n","    dt_bboxes = target_pred[\"boxes\"].numpy()\n","    dt_labels = target_pred[\"labels\"].numpy()\n","    dt_scores = target_pred[\"scores\"].numpy()\n","\n","    results = []\n","    for detection_id in range(len(dt_labels)):\n","        dt_bbox = dt_bboxes[detection_id, :]\n","        dt_label = dt_labels[detection_id]\n","        dt_score = dt_scores[detection_id]\n","\n","        detection_result_dict = {\"score\": dt_score}\n","\n","        max_IoU = 0\n","        max_gt_id = -1\n","        for gt_id in range(len(gt_labels)):\n","            gt_bbox = gt_bboxes[gt_id, :]\n","            gt_label = gt_labels[gt_id]\n","\n","            if gt_label != dt_label:\n","                continue\n","\n","            if intersection_over_union(dt_bbox, gt_bbox) > max_IoU:\n","                max_IoU = intersection_over_union(dt_bbox, gt_bbox)\n","                max_gt_id = gt_id\n","\n","        if max_gt_id >= 0 and max_IoU >= iou_threshold:\n","            detection_result_dict[\"TP\"] = 1\n","            gt_labels = np.delete(gt_labels, max_gt_id, axis=0)\n","            gt_bboxes = np.delete(gt_bboxes, max_gt_id, axis=0)\n","\n","        else:\n","            detection_result_dict[\"TP\"] = 0\n","\n","        results.append(detection_result_dict)\n","\n","    return results\n","\n","\n","def evaluate(model, test_loader, device):\n","    results = []\n","    model.eval()\n","    nbr_boxes = 0\n","    with torch.no_grad():\n","        for batch, (images, targets_true) in enumerate(test_loader):\n","            images = list(image.to(device).float() for image in images)\n","            targets_pred = model(images)\n","            targets_true = [\n","                {k: v.cpu().float() for k, v in t.items()} for t in targets_true\n","            ]\n","            targets_pred = [\n","                {k: v.cpu().float() for k, v in t.items()} for t in targets_pred\n","            ]\n","\n","            for i in range(len(targets_true)):\n","                target_true = targets_true[i]\n","                target_pred = targets_pred[i]\n","                nbr_boxes += target_true[\"labels\"].shape[0]\n","\n","                results.extend(evaluate_sample(target_pred, target_true))\n","\n","    results = sorted(results, key=lambda k: k[\"score\"], reverse=True)\n","\n","    acc_TP = np.zeros(len(results))\n","    acc_FP = np.zeros(len(results))\n","    recall = np.zeros(len(results))\n","    precision = np.zeros(len(results))\n","\n","    if results[0][\"TP\"] == 1:\n","        acc_TP[0] = 1\n","    else:\n","        acc_FP[0] = 1\n","\n","    for i in range(1, len(results)):\n","        acc_TP[i] = results[i][\"TP\"] + acc_TP[i - 1]\n","        acc_FP[i] = (1 - results[i][\"TP\"]) + acc_FP[i - 1]\n","\n","        precision[i] = acc_TP[i] / (acc_TP[i] + acc_FP[i])\n","        recall[i] = acc_TP[i] / nbr_boxes\n","\n","    return auc(recall, precision)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uAhZRzEEij2I"},"source":["<br>  \n","<br>  \n","  \n","## Часть 1.  \n","  \n","*Вес в общей оценке - 0.4*  \n","  \n","В данной части вам нужно решить задачу детектирования фруктов \"без классификации\". Для этого все три класса нужно объединить в один (нужно внести соотвествующие изменения в датасет)."]},{"cell_type":"markdown","metadata":{"id":"ggbYZdKYij2M"},"source":["### Задание 1.  \n","  \n","Примените обученные детекторы из [torchvision.models](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection) - Faster R-CNN, RetinaNet, SSD (можно только с одним backbone, можно все попробовать) - и оцените качество детекции на тестовом датасете. "]},{"cell_type":"markdown","metadata":{"id":"Xif5C9uMij2P"},"source":["### Задание 2.  \n","  \n","Обучите детекторы из задания выше на обучающем датасете, оцените качество на тестовом. При необходимости, подберите гиперпараметры - optimizer, lr, weight_decay etc.  \n","Выполните обучение в двух вариантах: со случайной инициализацией весов и с загрузкой весов уже обученной модели. Сравните качество. "]},{"cell_type":"markdown","metadata":{"id":"UZwLwnq0ij2Q"},"source":["### Задание 3.  \n","  \n","- Для лучшей модели оцените, как меняется качество на тестовых данных при изменении порога IoU.  \n","- Также добавьте порог для минимального значения score у предсказанных bounding box'ов, таким образом отсеивая предсказания с низким конфиденсом. Оцените, как меняется качество при изменении порога (порог IoU используйте 0.5). "]},{"cell_type":"markdown","metadata":{"id":"KxQolyEdij2S"},"source":["### Задание 4.  \n","  \n","Нарисуйте предсказанные bounding box'ы для любых двух картинок из __тестового__ датасета и каких-нибудь картинок из __интернета__ (релевантных - где есть эти фрукты - и тех, где этих фруктов нет)."]},{"cell_type":"markdown","metadata":{"id":"sByvAEkaij2T"},"source":["### Задание 5.  \n","  \n","Реализуйте и примените Non-maximum Suppression. Оцените (визуально и по метрикам), как его использование влияет на качество детекции.   \n","**NB:** Чтобы продемонстрировать эффективность применения алгоритма, возможно, нужно будет взять достаточно низкий порог конфиденса. "]},{"cell_type":"markdown","metadata":{"id":"NBWBc9IAij2V"},"source":["<br>  \n","<br>  \n","  \n","## Часть 2.  \n","  \n","*Вес в общей оценке - 0.4*  \n","  \n","Выполните задания 2-5 из предыдущей части для случая 3-х классов (как в изначальном датасете)."]},{"cell_type":"markdown","metadata":{"id":"f3o1nRpoij2W"},"source":["<br>  \n","<br>  \n","  \n","## Часть 3.  \n","  \n","*Вес в общей оценке - 0.2*  \n","  \n","Обучите модель для object detection для трех классов на __обучающем__ датасете и добейтесь PR AUC не менее __0.91__ на  __тестовом__.  \n","Баллы за задание вычисляются по формуле: __min(2, 2 * Ваш auc / 0.91)__."]},{"cell_type":"markdown","metadata":{"id":"JCw1c5ukij2X"},"source":["<br>  \n","<br>  \n","  \n","## Бонусные задания.  \n","  \n","1. При обучении используйте аугментации (в первую очередь пространственные) из [albumentations](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/) или [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html).  \n","2. Возьмите одну из [детекционных архитектур](https://paperswithcode.com/sota/object-detection-on-coco) (желательно, не старее 2020 года), у которой выложены тренировочный код и чекпоинты на гитхабе, обучите и провалидируйте ее на данных этой лабораторной. "]}]}